---
title: 被动读写锁
comments: true
tags:
  - GAP
  - rwlock
  - OS
categories: Sys
abbrlink: '9271'
date: 2020-03-22 15:35:14
---

Prwlock 为TSO架构系统提供了可扩展读者侧性能和写者低延迟，其关键在于多个无交互读者和一个不确定写者间基于version的共识协议。Prwlock 利用内存一致性的有界旧一致性(bounded staleness)来避免读者侧的原子操作和内存屏障，同时用消息传递(IPI)落伍的读者来限定写锁获取延迟。<!--more-->

## 背景知识

### 内存一致性

并发编程主要有两种编程范式：消息传递及共享内存。前者主要用于分布式系统，通过网络，采用消息的发送及接收的方式完成对共享数据的访问；而后者主要用于单机，通过内部互联总线，采用读写共享内存的方式完成对共享数据的访问。一致性模型为后者服务。

当一个处理器对共享变量（V）的本地副本进行了修改，而另一个处理器（PB）用于保存相应变量的缓存行因此而失效或被更新，则称 PB 看见了 PA 的内存更新（对变量 V 的修改）内存一致性模型规定了处理器能否在一个变量为指定处理器可见之前继续执行下去。

一种系统在每个处理器内部都设置了一块写缓冲区，用以吸收写指令，并异步地将这些写指令分别提交到内存控制器中。本处理器在读取内存时，如果检测到写缓冲区里已经存在针对同一地址的写了，那么它就直接解析这个写指令要写入的值并返回。这样能够很好地增加访存指令吞吐量。但在多处理器系统上，处理器很可能会从内存中读到过期的数据，因为相关的写操作可能仍缓冲在另一个处理器的缓冲区里，从而遭遇一致性问题。

#### 顺序一致性

最严格的一致性，当然性能也最差。确保程序顺序和原子性。程序顺序，即各处理器的操作保持程序指定的顺序；原子性，即各处理器能够同时看到访存指令的结果。这种最易理解，一致性长度也最高，但那些基于重排序的编译器优化都不能用了，比如代码移动、寄存器染色、子表达式消除。

> 本文中的访存（Memory Access）指令，指读存指令（Load）和写存指令（Store）

#### TSO(Total Store Order)

Intel x86采用TSO模型，各处理器看见所有写存指令的执行顺序都是一样的。

**程序顺序**上，先写后读(W->R)的顺序可以改变为先读后写(R->W)，但W->W(写操作)和R->RW(读取并赋值)的顺序不可改变。

**原子性**上，不能提前读取到别的处理器的写。即某个处理器修改了变量 x 后，别的处理器要么都看到这个修改，要么都没看到这个修改。但可以提前读取本处理器的写。即本处理器不必等待别的处理器看见本处理器的写操作，就可以继续运行下去。

同时TSO提供了增强一致性的 safety net ：

1. **RMW(read-modify-write)**：在 intel 64 架构中，即为 locked 指令。

2. **Memory barrier**：一种特别的屏障指令，其导致 CPU 或编译器强制约束内存指令，使之在屏障之前或之后发射。以 mfence 这个指令为例，其保证在程序顺序中，mfence 之前的所有访存指令，其结果都能为 mfence 之后的访存指令所见。

> 发射(issue)：在乱序处理器中指指令进入指令调度器；派发(dispatch)：指令进入执行单元。
> 一般将发射理解为已经开始执行了。

#### 有界旧一致性 bounded staleness

提交的写更新操作，不一定立即会被读操作读到，读写两者所冲突的区间形成了一个“不一致窗口”，我们允许该窗口存在，但window size应在一定限度范围内，而在窗口之外 bounded staleness 可以保证操作的全局按序。通常有两种size尺度，一种以版本个数K为限，此时window size指读操作读到的值和最新值的版本差max。一种以时间间隔T为限，此时window size指读操作可以读到最新值的时间差max。

### 读写锁 rwlock

rwlock 把对共享资源的访问者划分成读者和写者，读者只对共享资源进行读访问，写者则需对共享资源进行写操作。这种锁能提高并发性，在多处理器系统中它允许同时有多个读者来访问共享资源，最大可能的读者数为实际的逻辑CPU数。写者是排他性的，一个读写锁同时只能有一个写者或多个读者，但不能同时既有读者又有写者。

如果读写锁当前没有读者，也没有其它写者，那么写者可以立刻获得读写锁，否则它必须自旋在那里，直到没有任何写者或读者。如果读写锁没有写者，那么读者可以立即获得该读写锁，否则读者必须自旋在那里，直到写者释放该读写锁。

#### 大读者锁 brlock

大读者锁是读写锁的高性能版，读者可以非常快地获得锁，但写者获得锁的开销比较大。这种锁适合于读多写少的情况，它在这种情况下远好于读写锁。大读者锁的实现机制有两种：

* brlock1：每个线程都有互斥锁，一个读者仅需要获取本线程锁，但写者需要获得所有锁
* brlock2：使用一个由读者和写者共享的读者flags数组

当一个线程被抢占然后移到另一个core的时候，brlock容易发生死锁，所以brlock常用在禁止抢占的环境中。

## 设计框架

### 设计背景

典型的rwlock依赖于原子指令来协调读者和写者。在多处理器上，一条原子指令暗含了一次内存屏障，这样可以保证读者读到的是最后一位写者的最新数据。但当不存在任何写程序时，这样的内存屏障是不必要的，读程序之间没有内存顺序依赖性，此类不必要的内存屏障可能会导致大量开销。

**Message passing is not prohibitively expensive:**
常见多核处理器类似于分布式系统，每个核都有自己的内存层次结构。每个内核本质上都使用消息传递来与其他内核通信，但硬件设计人员添加了一个抽象（缓存一致性）来模拟共享内存接口。但由于连贯性消息的序列化，共享有竞争的cache开销可能远超像处理器间中断（IPI）之类的显式消息传递。此外，将多个IPI传递到不同的内核可以并行进行。

> 处理器间中断（Inter-Processor Interrupt）是种特殊类型的中断，即在多核系统中，允许一个CPU向系统其他的CPU发送中断信号，可能要求采取的行动：刷新其它CPU的内存管理单元缓存、停机（当系统被一个处理器关闭时）

**Bounded staleness without memory barriers：**
在rwlock中，一个写者需要与所有读者达成 consensus 才能获取该锁。因此，写者必须让所有读者看到其当前状态才能继续进行。典型的rwlock使用内存屏障以确保读者写者的版本更新按顺序对彼此可见，但这样开销太大，那么能否不用内存屏障？事实上一般处理器如x86-64，通常在很短的时间内其他内核就可以看到多个内存更新。基于此采取微基准测试：重复写入一个内存位置，并在随机延迟后在另一个内核上读取该位置，然后收集看到过时值的读者的时间间隔，发现大多数读者其实可以在很短的时间内看到写者的更新（因为处理器由于其有限的大小而将主动刷新其存储缓冲区）

**Memory barrier not essential for mutual exclusion:**
现代处理器为了性能考虑不会按序执行指令，存在一个存储缓冲区，以使处理器在write cache未命中后继续执行，但这也导致了较弱的内存一致性。为了实现正确的互斥，通常使用性能开销较大的同步机制（例如内存屏障）来对管道进行序列化并刷新存储缓冲区。事实证明，在TSO机器上的所有执行中，都不可能构建一种满足互斥，无死锁且避免原子指令和内存屏障的算法。尽管prwlock读者不包含显式的内存屏障，因此似乎违反了该“顺序法则”，但prwlock使用 IPI 序列化了针对写者的读者执行，并且IPI处理具有与内存屏障相同的效果。

### 基本设计

**Consensus using bounded staleness:**
Prwlock 将version变量引入锁结构。每个写者都会自增version，并等待所有读者看到新的version后才继续写。 

写者：lock(writer) ; ver++;
读者：while(writer!=FREE) status[my_id] = ver; 
写者：for_each (id) while(status[id] < ver); 

但如果某个读者id再也不进入临界区，那status[id]会永远小于ver，写者会一直陷入等待，无法进入临界区。 其次，读者可能会从一个core迁移到另一个core，但可能并不会更新离开的core。 

**Handling straggling readers:** 
为解决上述问题，prwlock引入了基于消息的共识协议，以使写者在必要时用IPI主动向读者发送请求。

写者：lock(writer) ; ver++;
读者：while(writer!=FREE) status[my_id] = ver; 
写者：for_each (id) while(status[id] < ver) **IPI_request_ to_reader(id)**;

但如果允许读者在读取侧临界区中睡眠的话，睡眠状态的读者可能会错过请求，因此可能会无限阻塞写者。

**Supporting sleeping readers:** 
为了解决读者睡眠问题，prwlock引入了传统的计数器，让写者确认此时有无睡眠读者。

Prwlock跟踪两种类型的读者：被动读者和主动读者。读者开始时都设为passive，但在睡眠前会从passive转换为active。在此转换期间，共享的counter会自增。当active读者释放其锁定后，counter自减。写者使用该counter来确定是否有任何活动的读者。由于读者在读取器侧临界区睡眠很少，因此prwlock总体具有良好的性能。

### 算法设计

为了简化说明，我们假设这些功能中只有一个锁，并且禁止抢占，以便它们可以安全地使用每个CPU状态。

#### Read-side algorithm

每个per-core reader status structure（st）分布式跟踪passive reader，st会记住最新查看的version以及每个内核上prwlock的被动状态。**FREE, PASSIVE两种状态体现在st.reader上，ACTIVE状态体现在lock.active上。**

要锁读者锁，要先设置状态为PASSIVE，再检查写者锁（*否则会存在一个时间窗口：读者明明观测到写者锁处于FREE，但并不能获得读者锁。如果在此时间窗口中传递了共识消息（例如IPI），则写者将成功获取锁并进入临界区，这违反了rwlock的语义*）如果读者发现此锁已被写者锁定(lock.writer != FREE)，则应释放被动锁定，将状态设置为FREE，等待写者解锁并重试。

要解锁读者锁，只需检查该锁是否处于PASSIVE并相应地将其解锁（*ReadUnlock*）。因此，在TSO体系结构上的读者公共路径中，不需要原子指令/内存障碍。此外，PASSIVE读者不会相互通信，从而保证了读者的可伸缩性和较低的延迟。

如果读者被调度时正在PASSIVE模式持有锁，则应通过增加主动计数器（*ScheduleOut*）转换为ACTIVE锁。

**ReadLock(lock)**

```
st ← PerCorePtr(lock.rstatus, CoreID); 
st.reader ← PASSIVE; 
while lock.writer != FREE do 
	st.reader ← FREE; //释放被动锁定
	st.version ← lock.version; 
 	WaitUntil(lock.writer == FREE); 
 	st ← PerCorePtr(lock.rstatus, CoreID); 
 	st.reader ← PASSIVE; 
/* Barrier needed here on non-TSO architecture */; 
```
**ReadUnlock(lock)** 

```
st ← PerCorePtr(lock.rstatus, CoreID);
if st.reader = PASSIVE then st.reader ← FREE;
else AtomicDec(lock.active);
/* Barrier needed here on non-TSO architecture */;
st.version ← lock.version;
```
**ScheduleOut(lock)**

```
st ← PerCorePtr(lock.rstatus, CoreID); 
if st.reader = PASSIVE then AtomicInc(lock.active);
st.reader ← FREE;
st.version ← lock.version;
```

#### Write-side algorithm

写者锁的获取可分为两个阶段。一个写者首先锁定writer mutex，然后增加version以进入阶段1。然后，它将检查当前域中的所有联机cores，以确认cores是否已查看最新version。如果ok，则说明读者都在线，即知道此锁已被写者锁定，并在写者释放锁前不会获得读者锁。对于看不到最新version的内核，写者发送IPI并询问其状态。收到IPI后，未锁定的读者将通过更新其本地version（*Function Report*）向写者报告。锁定的读者在离开读取侧临界区或进入睡眠状态后将稍后报告。在报告了所有核心后，所有passive读者都达成了共识。然后，写者进入阶段2。在此阶段，写者只需等待所有活动的读者退出即可。

写者可以将锁直接传递给之后的writer，而无需再次达成共识（line2-4 in WriteLock and line1 in WriteUnlock）

**WriteLock(lock)**

```
lastState ← Lock(lock.writer);
if lastState = PASS then return;
/* Lock passed from another writer */
newVersion ← AtomicInc(lock.version);
coresWait ← /0;
for ID ∈ AllCores do 
	if Online(lock.domain, ID) ∧ ID != CoreID then 
		if PerCorePtr(lock.rstatus, CoreID).version != newVersion then 
			AskForReport(ID);
			Add(ID, coresWait);
for ID ∈ coresWait do 
	while PerCorePtr(lock.rstatus, CoreID).version != newVersion do Relax();
while lock.active != 0 do Schedule();
```
**WriteUnlock(lock)**

```
if SomeoneWaiting(lock.writer) then Unlock(lock.writer, PASS); 
else Unlock(lock.writer, FREE);
```
**Report(lock)**

```
st ← PerCorePtr(lock.rstatus, CoreID);
if st.reader != PASSIVE then st.version ← lock.version;
```

#### Correctness on TSO architecture

rwlocks与其他较弱的同步原语之间的主要区别在于，rwlocks在读者写者间有强大的可见性保证。
一旦读者看到了FREE的prwlock，可以确定：1）该FREE由直接的前一个写者设置；2）由于在TSO架构下按顺序可见内存写入，因此以前的写者所做的更新也应对该读者可见；3）在该读者退出前，没有写者可以进入临界区。
这三个属性共同确保读者始终可以看到受prwlock保护的共享数据的最新一致版本。此外，由于所有读者在获取写者锁期间都明确报告了最新版本，因此还可以保证写者查看读者st的所有更新。
在非TSO架构上，读者算法中需要两个额外的存储屏障。第一个确保读者在快速路径中获得锁定后，可以看到共享数据的最新版本。第二个是在释放读者锁之前使写者可以看到读者的内存更新。

### OS内核集成

将prwlock集成到OS内核中存在几个问题。首先，prwlock的范围可以是全局范围的，也可以是进程范围的，并且每个范围中可能有多个prwlock。每个prwlock可以由多个任务共享。为了减少读者写者之间的消息，prwlock使用锁域抽象来将可以达成共识的相关prwlocks归为一组。域跟踪当前正在执行prwlock相关任务的CPU内核。

#### Domain Online/Offline

在操作系统执行期间，可能会关闭一组prwlocks的范围。例如，对于保护进程的地址空间结构的一组锁，可以在地址空间切换期间关闭该结构。在这种情况下，prwlock使用域抽象来避免不必要的共识消息。域维护从cores到其online/offline状态的映射。只有活动域中的CPU内核才需要发送消息。

当域将在core上联机时`coreSt ← PerCorePtr(dom.cores, CoreID); `，它只需设置映射`coreSt.online = TRUE; `然后执行内存屏障`MemoryBarrier();`。由于写者总在检查域之前设置其状态，因此可以保证写者可以看到新联机的内核，或者该内核上新来的读者可以看到写者正在获取锁。在任何一种情况下，rwlock语义都会保留。为了正确地使域从core中offline，在更改域之前还需要一个内存屏障，以确保offline前所有操作都对其他cores可见。
对于与进程相对应的域，prwlock使域在上下文切换前后联机/脱机。在域外时，读者必须在较慢的ACTIVE状态中获取所有prwlock。我们选择保留锁定用户的选择，因为他们可能会对工作量有更多的了解。 

#### Task Online/Offline

可以将task（例如线程）上下文切换到其他任务，也可以将任务从一个核迁移到另一核。 prwlock使用联机/脱机任务来处理此类操作。当持有PASSIVE模式下prwlock的task即将切出时，如果prwlock为passive reader，将改为ACTIVE并增加active reader counter。这样可以确保写者一直处于等待，直到再次调度此任务，离开临界区继续执行。任务被调度再次联机时，无需执行任何操作。

#### DownGrade/Upgrade

典型的操作系统通常支持将rwlock从写模式降级到读模式，并从读模式升级到写模式。通过将当前任务设置为读模式，然后在写模式下释放锁，Prwlock同样支持锁降级。与传统rwlock不同，在少数情况下，将prwlock从读模式升级到写模式，由于缺少读者数量的确切消息，可能开销会更大。要将锁从读模式升级到写模式，prwlock会尝试在读取侧临界区以写入模式获取锁，但是获取锁时，读者的数量要少一个（不包括升级读者本身）。

### 用户级支持

用户空间实现主要障碍是无法在用户空间获取锁期间禁用抢占。
为了解决此问题，prwlock依赖于某些内核支持。背后的想法很简单：当有必要对每个内核状态执行任何操作时，prwlock进入内核并由内核处理。我们没有使用每核数据结构st来维护被动读者状态，而是在用户空间中引入了每线程数据结构。每个线程都应在执行锁定操作前将其实例注册到内核，因为任何时候内核上都只有一个线程在运行。这样的每线程数据结构类似于内核算法中使用的每核数据结构。

出于性能方面的考虑，读者临界区应完全位于用户空间内，否则syscall开销将破坏prwlock的短延迟优势。
在prwlock中，被动锁保持为每核状态，而主动锁则保持在共享计数器中；检查和更改被动锁定模式应自动完成。
ReadUnlock算法会在开头2-3行检查读者是否为PASSIVE，如果是，则通过将状态设置为FREE来释放被动锁定。如果线程在第2行和第3行之间被抢占，则该锁可能会转换为活动锁，并且活动计数会增加。稍后进行调度时，由于之前已经做出决定，因此活动计数不会减少。结果，rwlock变得不平衡，并且写者再也无法获取该锁。

为了克服这个问题，我们在每线程数据结构中添加了抢占检测字段。读取器首先将状态设置为PASSIVE，然后检查是否在被动锁定时被抢占。如果是这样，它将减少活动计数器，因为该锁现在是活动锁。对于写端算法，由于不可能在用户空间中发送IPI，因此几乎所有写者都应进入内核以获取锁。 不过与写者锁获取时间相比，内核和用户空间之间的模式切换成本通常可以忽略不计。

**Function ReadUnlock(lock) for user-level prwlock**
```
st ← PerThreadPtr(lock.rstatus);
st.reader ← FREE;
if st.preempted then 
	AtomicDec(lock.active);
	st.preempted ← FALSE;
st.version ← lock.version;
```
**Function ScheduleOut(lock)**

```
st ← PerThreadPtr(lock.rstatus);
if st.reader = PASSIVE then 
	AtomicInc(lock.active);
	st.preempted ← TRUE;
	st.reader ← FREE;
st.version ← lock.version;
```

### 性能分析

* **内存屏障：**在读取侧临界区的通用路径中，没有写者时prwlock不需要任何内存屏障。唯一需要的内存屏障是当CPU内核即将离开锁定域时，例如，切换到另一个任务并使当前的锁定域脱机或联机。但域联机/脱机操作在典型执行中很少见。因此，通常情况下prwlock具有良好的性能可伸缩性。

* **写者成本：**使用IPI可能会显着增加写入成本，但IPI和模式切换的成本很小。此外，由于写者通常需要等待所有读者都离开临界区，因此开销可以忽略不计。与传统rwlock相比，当前在读取侧临界区执行的读者越多，写入完成共识并获得写入模式锁定的速度就越快。因为读者很可能会看到写者，因此会立即报告。这样的功能非常适合 rwlocks（读者多于写者）

* **空间开销：**由于prwlock本质上是分布式rwlock，因此它需要O（n）空间用于锁实例。
  当前实现需要每个锁每个内核12个字节（版本8个，读取器状态4个），以使性能最大化。也可以将7位version和1位status打包到一个字节中以节省空间。需要另外几个字节来存储写者状态，其确切大小取决于特定写者使用的同步机制。此外，每个内核需要额外的1个字节来存储域online状态以支持锁定域抽象。通过使用Linux内核的按CPU存储机制，锁的按CPU状态可以与其他按CPU状态字打包到同一缓存行中。与其他可扩展的rwlock算法相比，prwlock施加了相似或更低的空间开销。

* **内存一致性模型要求：**由于prwlock依赖于一系列的内存操作事前发生关系，因此它要求内存存储操作被执行并按发布顺序对其他用户可见（TSO一致性）。像x86-64，SPARC和zSeries等常用处理器都是这个架构。

## 分布式并行唤醒

### 集中式顺序唤醒的问题

睡眠/唤醒是一种常见的OS机制，该机制允许任务暂时睡眠以等待特定事件发生（例如I / O事件）。Linux操作系统使用共享队列来保存所有等待的任务。通常是signaling task负责唤醒所有等待的任务。为此，signaling task首先使任务从共享任务队列中出队，然后执行一些操作以准备唤醒任务。接下来，调度程序为任务选择一个core，然后将该任务插入cpu运行队列中。最后，调度程序将重新调度的IPI发送到目标内核，以便唤醒的任务可能有机会被调度。内核将重复发送IPI，直到重新安排了所有唤醒的任务。

这种集中的顺序唤醒机制存在多个问题。一，共享等待队列可能会成为瓶颈，因为尝试进入睡眠的多个内核可能会争用该队列。因此，我们的第一步涉及使用无锁唤醒队列，以便可以减轻锁争用。但这只能稍微提高性能。
进一步调查发现，主要的性能可伸缩性问题来自级联的唤醒现象。当写者离开其写侧临界区时，它需要唤醒所有等待它的读者。由于有多个读者在等待写者，写者会顺序唤醒所有读者。因此等待时间与读者数量成线性增长。

### 分布式并行唤醒

为了加快此过程，prwlock在内核之间分配了唤醒任务的职责。一般来讲，这会带来优先级倒置的风险，但所有读者始终具有相同优先级，所以就不存在什么问题了。

![被动读写锁_parallel_wakeup.png](https://i.loli.net/2020/03/25/ECLQfDs536FGhmk.png) 

上图展示了分散并行唤醒中使用的关键数据结构。每个内核维护一个唤醒队列（PWake-queue），以保持在此类队列中休眠的任务，每个任务均在唤醒条件字下休眠。

当任务从运行态进入睡眠状（步骤1）时，该任务将从CPU运行队列Run-queue中删除，并插入到CPU唤醒队列PWake-queue中。在进入调度程序之前，如果内核指示存在待处理的请求（例如，通过检查唤醒计数器），则每个内核都先查看PWake-queue中是否存在tasks meet their conditions，如果存在，它将被插入Run-queue。由于所有操作都在每个core的本地完成，所以不需要原子操作和内存屏障。在检查了PWake-queue之后，每个内核将执行其调度程序（步骤2）以选择要执行的任务（步骤3）

为了解决集中式顺序唤醒的问题，唤醒机制允许每个空闲core使用monitor & mwait instructions to sleep on a global word（步骤4）当写者完成工作并发出信号以唤醒其等待的任务时，写者touch the word以唤醒空闲的内核，然后该内核将开始检查是否应唤醒唤醒队列中的任何任务。

